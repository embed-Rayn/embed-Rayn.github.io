제목: QASA: Advanced Question Answering on Scientific Articles
[PDF_LINK](https://proceedings.mlr.press/v202/lee23n.html)

저자:  Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang,  Jaehyeon Kim, Hong-in Lee, Moontae Lee
출판사: ICML PMLR
### Abstract
- 논리적 추론: 지적 사고의 핵심
- QA 대부분은 깊이 있는 이해를 요구하지 얕은 질문 응답
- 이 논문
	- “추론이 연상적 사고와 논리적 추론으로 구성된다”는 이론을 기반
	- QASA 벤치마크 제안
	- 표면적, 테스트, 심층 질문의 세 가지 유형의 질문을 포함
	- AI 및 ML 논문을 대상으로 하는 1798개의 새로운 질문 응답 쌍으로 구성
- 실험 결과
	-  QASA가 최신 INSTRUCTGPT(ChatGPT 이전 버전) 크게 능가
### 1. Introduction
1. **추론의 중요성**
    - 추론은 지적 사고의 핵심 요소임.
    - 이중 과정 모델: 연상적 사고와 논리적 추론의 두 단계로 구성됨.
2. **기존 QA 시스템의 한계**
    - 대부분의 QA 데이터셋은 사실적 질문에 중점을 두고 있음.
    - 깊이 있는 이해를 요구하는 질문에 대한 대응 부족.
3. **QASA 벤치마크의 제안**
    - AI 및 ML 분야의 과학 논문을 대상으로 함.
    - 표면적 질문, 테스트 질문, 심층 질문의 세 가지 유형의 질문을 포함한 1798개의 새로운 질문 응답 쌍으로 구성됨.
### 2. Related Work 개요
1. **QA for Academic Research Papers**
    - emrQA, BioRead, BioMRC: 자동으로 QA 예제를 생성하나, 실제 시나리오 반영이 부족.
    - QASPER: 5K QA로 구성된 NLP 도메인 논문 대상. 대부분 얕은 질문으로 구성되며, 단답형 답변 비율이 높음.
    - QASA와의 차이점: QASPER는 주로 얕은 질문에 집중하나, QASA는 다양한 질문 유형과 심층적 이해를 요구.
2. **Open-domain Long-form QA**
    - ELI5: Reddit 포럼의 질문을 기반으로 하여, 웹 문서에서 문장을 추출하여 증거로 제공.
    - ASQA: 여러 문단에 걸쳐 서브 질문에 대한 답변 요구.
    - QASA와의 차이점: QASA는 명확한 증거와 체계적인 구성 요소를 포함하여 보다 포괄적인 답변을 생성.
3. **Query-focused Multi-Document Summarization (qMDS)**
    - QMSum, Squality, AQuaMuSe: 여러 문서에서 답변을 찾는 것을 목표로 하나, 자동 생성된 문단 사용으로 인한 노이즈와 부족한 문맥.
    - QASA와의 차이점: 인간이 주석한 증거와 종합적인 답변 생성에 중점. 다중 증거를 포함한 풍부한 장문의 답변을 생성.
#### Table 1. 기존 데이터셋과 QASA의 비교

| Method          | Associative selection | Evidential rationale-generation | Systematic composition |
| --------------- | --------------------- | ------------------------------- | ---------------------- |
| QASPER          | ✓                     | ✗                               | ✗                      |
| ELI5            | ✗                     | ✗                               | ✗                      |
| ASQA            | ✓                     | ✗                               | ✓                      |
| AQuaMuSe        | ✗                     | ✗                               | ✓                      |
| QASA (**ours**) | ✓                     | ✓                               | ✓                      |
### 3. Proposed Task
1. **새로운 질문 응답 작업 제안**
    - 과학 논문에 대한 질문에 여러 증거 단편을 기반으로 답변 제공.
    - 질문(q), 답변(a), 논문의 문단(P)로 구성.
2. **다단계 서브태스크**
    - 연상적 선택(Associative Selection): 여러 문단에서 질문에 대한 증거 문단을 선택.
    - 증거 이유 생성(Evidential Rationale-Generation): 선택된 문단에서 증거 이유 생성.
    - 체계적 구성(Systematic Composition): 모든 증거 이유를 종합하여 최종 답변 작성.
3. **연상적 선택**
    - 여러 문단 중 *질문과 관련된 증거 문단을 추출*.
    - 질문 q와 문단 P = {p1, ...pN}을 주고, 증거 문단 P̄ = {p̄1, ...p̄k} 선택.
4. **증거 이유 생성**
    - *선택된 각 문단에서 최종 장문의 답변에 포함될 수 있는 증거 이유 생성*.
    - 증거 이유: 주된 답변, 부연 설명, 보조 정보 등으로 구성.
5. **체계적 구성**
    - *모든 증거 이유를 종합하여 최종 답변 a 작성*.
    - 중복된 텍스트를 제외하고 모든 중요한 증거를 포함하여 포괄적인 답변 작성.
### 4. Building the QASA Dataset 
1. **Preliminary Study**
    - 목적: 논문 독자들이 어떤 종류의 질문을 하는지 식별.
    - 방법: Think-aloud study(N=10)를 통해 127개의 질문 분석.
    - 결과: 질문의 67%가 이중 단계의 추론을 필요로 함.
2. **Schema 설계**
    - 교육 도메인의 질문 분류 문헌을 참고하여 논문 읽기 맥락에 맞게 스키마 설계.
    - 질문 유형: 표면적 질문, 테스트 질문, 심층 질문으로 나누어 다양한 수준의 추론을 요구.
3. **Papers 수집**
    - S2ORC와 arXiv의 공개 접근 AI/ML 논문 수집.
    - 필터링 기준: 2015년 이후 출판, 100회 이상 인용된 논문.
4. **데이터 수집 세션**
    - Reader Session: 일반 독자들에게서 Q&A 수집.
    - Author Session: 저자가 자신의 논문에 대한 질문 작성 및 주석.
5. **질문 유형 분포**
    - 표면적 질문: 30.7%
    - 테스트 질문: 30.0%
    - 심층 질문: 39.4%
6. **Evidential Rationale 분포**
    - 질문의 12%는 증거 이유가 없으며, 평균 1.67개의 증거 이유 필요.
    - 표면적 질문: 1.73개, 테스트 질문: 1.66개, 심층 질문: 1.63개.
### 5. QASA Approach
1. **QASA 접근 방식 개요**
    - 연구 논문에 대한 QA 접근 방식을 제안.
    - 다단계 추론이 필요하며, 이를 위해 연상 선택, 증거 이유 생성, 체계적 구성의 세 단계로 분류.
2. **다단계 QA 시스템**
    - 사전 처리 단계: 검색 모델을 사용해 논문 전체에서 관련 문단을 상위 10개로 축소.
-    - *연상 선택(Associative Selection)*: 질문과 관련된 문단을 선택.
```
Is there a rationale or answer to this question in this context?
Context: {paragraph}
Question: {question}
```

- [1] 예시
```
Is there a rationale or answer to this question in this context?
문맥: "실험 결과, 합성 데이터셋을 사용할 때 모델 정확도가 크게 향상되었습니다." 
질문: "합성 데이터셋이 모델 정확도를 향상시키나요?"
```
```
Is there a rationale or answer to this question in this context?
문맥: "저자들은 주의 메커니즘이 다양한 NLP 작업에서 LSTM 모델보다 우수하다고 주장합니다." 질문: "주의 메커니즘이 NLP 작업에서 LSTM 모델보다 우수한가요?"
```

-    - *증거 이유 생성(Evidential Rationale-Generation)*: 선택된 문단에서 증거 이유를 생성.
- [1] 예시
```
질문에 대한 답변이나 근거를 문맥에 기반하여 작성하십시오.
문맥: "실험 결과, 합성 데이터셋을 사용할 때 모델 정확도가 크게 향상되었습니다." 
질문: "합성 데이터셋이 모델 정확도를 향상시키나요?" 
답변: 합성 데이터셋을 사용했을 때 모델의 정확도가 크게 향상되었다는 실험 결과가 있습니다. 이는 합성 데이터셋이 모델 성능에 긍정적인 영향을 미친다는 것을 시사합니다.
```
```
질문에 대한 답변이나 근거를 문맥에 기반하여 작성하십시오.
문맥: "저자들은 주의 메커니즘이 다양한 NLP 작업에서 LSTM 모델보다 우수하다고 주장합니다."
질문: "주의 메커니즘이 NLP 작업에서 LSTM 모델보다 우수한가요?"
답변: 저자들은 주의 메커니즘이 다양한 NLP 작업에서 LSTM 모델보다 성능이 우수하다고 주장하고 있습니다. 이는 주의 메커니즘의 성능이 LSTM 모델을 능가한다는 것을 의미합니다.

```
-    - *체계적 구성(Systematic Composition)*: 모든 증거 이유를 종합하여 최종 답변을 작성.
- [1] 예시
```
주어진 질문에 대해 여러 증거를 종합하여 포괄적이고 일관된 답변을 작성하십시오.
증거들: 
1. "합성 데이터셋을 사용할 때 모델의 정확도가 크게 향상되었습니다."
2. "더 큰 데이터셋은 모델의 표현력을 증가시키는 데 기여할 수 있습니다."
질문: "합성 데이터셋이 모델 정확도를 향상시키나요?"
답변: 합성 데이터셋을 사용했을 때 모델의 정확도가 크게 향상되었으며, 더 큰 데이터셋이 모델의 표현력을 증가시켜 성능을 향상시킬 수 있습니다. 따라서 합성 데이터셋은 모델 정확도를 높이는 데 유용합니다.
```
```
주어진 질문에 대해 여러 증거를 종합하여 포괄적이고 일관된 답변을 작성하십시오.
증거들:
1. "주의 메커니즘이 다양한 NLP 작업에서 LSTM 모델보다 성능이 우수합니다."
2. "주의 메커니즘은 더 높은 정확도를 제공합니다."
질문: "주의 메커니즘이 NLP 작업에서 LSTM 모델보다 우수한가요?"
답변: 주의 메커니즘이 다양한 NLP 작업에서 LSTM 모델보다 성능이 우수하다는 주장이 있으며, 이는 주의 메커니즘이 더 높은 정확도를 제공하기 때문입니다. 따라서 주의 메커니즘은 NLP 작업에서 LSTM 모델보다 더 나은 성능을 발휘합니다.
```
1. **대형 언어 모델(LM) 훈련**
    - 다양한 하위 작업을 포함한 다중 작업 명령을 통해 대형 언어 모델을 미세 조정.
    - T5, T0, FLAN-T5, GALACTICA 모델을 사용.
2. **훈련 데이터**
    - 공공 데이터 및 합성 데이터를 활용.
    - 하위 작업별로 QASPER, ASQA, ELI5 데이터셋 사용.
    - InstructGPT를 통해 합성 데이터 생성.
3. **실험 결과**
    - QASA 접근 방식이 InstructGPT보다 우수한 성능을 보여줌.
    - *증거 이유 생성 단계*가 성능 향상에 중요한 역할을 함.
    - FLAN-T5 모델이 전체 QA 작업에서 가장 우수한 성능을 보임.
    - QASA Experimental Results
#### Table 4. The results of full-stack QA systems on QASA.

| Method(*Pretrained*) | Full-stack QA (R-1) | Full-stack QA (R-2) | Full-stack QA (R-L) |
| -------------------- | ------------------- | ------------------- | ------------------- |
| GALACTICA (6.7B)     | 15.56               | 3.65                | 11.44               |
| T5 (3B)              | 9.83                | 0.58                | 8.01                |
| T0 (3B)              | 15.6                | 4.28                | 12.15               |
| FLAN-T5 (3B)         | 22.48               | 9.52                | 18.45               |
| INSTRUCTGPT (175B)   | 27.11               | 11.9                | 19.75               |

| Method(*Finetuned*) | Full-stack QA (R-1) | Full-stack QA (R-2) | Full-stack QA (R-L) |
| ------------------- | ------------------- | ------------------- | ------------------- |
| GALACTICA (6.7B)    | 20.93               | 6.16                | 15.01               |
| T5 (3B)             | 26.66               | 11.45               | 20.73               |
| T0 (3B)             | <u>29.75</u>        | <u>13.13</u>        | <u>22.75</u>        |
| FLAN-T5 (3B)        | **32.22**           | **14.62**           | **24.53**           |
| w/o Rationale Gen   | 27.73               | 11.31               | 19.32               |

### 6. Experiment 개요
1. **Experimental Setting**
    - 평가 방식: 세부 작업(연상적 선택, 증거 이유 생성, 체계적 구성)과 전체적인 QA 작업을 자동 및 인간 평가를 통해 평가.
    - 평가 메트릭: 연상적 선택 작업에 대해 Precision(P), Recall(R), F1 점수 측정. 증거 이유 생성 및 답변 구성 작업에는 ROUGE 점수 사용.
    - 오라클 컨텍스트 제공: 세부 작업 평가 시 오라클(또는 골드) 컨텍스트 제공, 작업별로 독립적 평가 가능.
2. **Main Results**
    - 최고의 사전 훈련된 언어 모델: INSTRUCTGPT(175B)가 모든 작업에서 가장 우수한 성능을 보임.
    - FLAN-T5의 우수성: 미세 조정된 FLAN-T5가 전체 QA 작업에서 최고의 성능을 보임.
    - 훈련 자원 효과: 데이터셋별 FLAN-T5 성능 비교 시 ASQA-ONLY 훈련 모델이 답변 구성 작업에서 최고 성능을 보였고, GPT AUG 훈련 모델이 증거 이유 생성 작업에서 성능 향상.
3. **Human Evaluation**
    - 평가 방식: 두 개의 답변(하나는 QASA, 다른 하나는 INSTRUCTGPT)을 비교하여 인간 평가자가 더 나은 답변 선택.
    - 평가 기준: Groundedness, Completeness, Specificity, Fluency 기준으로 평가.
    - 결과: QASA 접근 방식이 INSTRUCTGPT보다 전반적으로 우수하다는 평가.
4. **Error Analysis**
    - 오류 분류: 모델 오류를 E1부터 E5까지 다섯 가지 범주로 분류.
    - 주요 오류: E1(답변 불가능으로 잘못 예측), E2(관련 없는 내용 생성), E3(명시적 답변 실패), E4(근거 부족), E5(답변 불완전).